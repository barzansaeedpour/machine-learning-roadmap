{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Label classification metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a predictive modeling problem that involves outputting a class label given some input\n",
    "\n",
    "It is different from regression tasks that involve predicting a numeric value.\n",
    "\n",
    "Typically, a classification task involves predicting a single label. Alternately, it might involve predicting the likelihood across two or more class labels. In these cases, the classes are mutually exclusive, meaning the classification task assumes that the input belongs to one class only.\n",
    "\n",
    "Some classification tasks require predicting more than one class label. This means that class labels or class membership are not mutually exclusive. These tasks are referred to as multiple label classification, or multi-label classification for short.\n",
    "\n",
    "**In multi-label classification, zero or more labels are required as output for each input sample, and the outputs are required simultaneously**. The assumption is that the output labels are a function of the inputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we have five samples with four labels. y_true is the actual labels and y_pred is the predicted labels by any classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "\n",
    "y_true = np.array([[0,1,0,1],\n",
    "                   [0,1,1,0],\n",
    "                   [1,0,1,0],\n",
    "                   [1,1,1,1],\n",
    "                   [0,0,1,1]])\n",
    "\n",
    "y_pred = np.array([[0,1,1,0],\n",
    "                   [0,1,1,0],\n",
    "                   [0,1,0,1],\n",
    "                   [0,0,0,0],\n",
    "                   [0,0,1,0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./files/multi-label-metrics/exact_match_ratio.png\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this formula is defined as accuracy_score in sklearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Ratio: 0.2\n",
      "sklearn - Exact Match Ratio: 0.2\n"
     ]
    }
   ],
   "source": [
    "def EMR(y_true,y_pred):\n",
    "   MR = np.all(y_pred == y_true, axis=1).mean() \n",
    "   return MR\n",
    "print('Exact Match Ratio:', EMR(y_true,y_pred))\n",
    "print('sklearn - Exact Match Ratio: {0}'.format(sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./files/multi-label-metrics/01_loss.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1Loss:  0.8\n"
     ]
    }
   ],
   "source": [
    "def _01Loss(y_true,y_pred):\n",
    "    return np.any(y_true != y_pred, axis=1).mean() \n",
    "print(\"0/1Loss: \", _01Loss(y_true,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./files/multi-label-metrics/accuracy.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36666666666666664\n"
     ]
    }
   ],
   "source": [
    "def Accuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        temp += sum(np.logical_and(y_true[i], y_pred[i])) / sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "print(Accuracy(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./files/multi-label-metrics/hamming_loss.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss: 0.55\n",
      "Hamming loss: 0.55\n"
     ]
    }
   ],
   "source": [
    "def Hamming_Loss(y_true, y_pred):\n",
    "    temp=0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        temp += np.size(y_true[i] == y_pred[i]) - np.count_nonzero(y_true[i] == y_pred[i])\n",
    "    return temp/(y_true.shape[0] * y_true.shape[1])\n",
    "    \n",
    "print('Hamming loss:' , Hamming_Loss(y_true, y_pred))\n",
    "print('Hamming loss: {0}'.format(sklearn.metrics.hamming_loss(y_true, y_pred))) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./files/multi-label-metrics/precision.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Precision: 0.4\n"
     ]
    }
   ],
   "source": [
    "def Precision(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if sum(y_true[i]) == 0:\n",
    "            continue\n",
    "        temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_true[i])\n",
    "    return temp/ y_true.shape[0]\n",
    "\n",
    "print('Precision:', Precision(y_true, y_pred))\n",
    "print('Precision: {0}'.format(sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, average='samples')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./files/multi-label-metrics/recall.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.5\n",
      "Recall: 0.5\n",
      "Recall: 0.4\n",
      "Recall: 0.3333333333333333\n",
      "Recall: 0.3333333333333333\n",
      "Recall: 0.29166666666666663\n"
     ]
    }
   ],
   "source": [
    "def Recall(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if sum(y_pred[i]) == 0:\n",
    "            continue\n",
    "        temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_pred[i])\n",
    "    return temp/ y_true.shape[0]\n",
    "    \n",
    "print('Recall:', Recall(y_true, y_pred))\n",
    "print('Recall: {0}'.format(sklearn.metrics.precision_score(y_true=y_true, y_pred=y_pred, average='samples',zero_division = 0)))\n",
    "print('Recall: {0}'.format(sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, average='samples',zero_division = 0)))\n",
    "print('Recall: {0}'.format(sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, average='weighted',zero_division = 0)))\n",
    "print('Recall: {0}'.format(sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, average='micro',zero_division = 0)))\n",
    "print('Recall: {0}'.format(sklearn.metrics.recall_score(y_true=y_true, y_pred=y_pred, average='macro',zero_division = 0)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./files/multi-label-metrics/f1_measure.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Measure: 0.4333333333333333\n",
      "F1 Measure: 0.4333333333333333\n"
     ]
    }
   ],
   "source": [
    "def F1Measure(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if (sum(y_true[i]) == 0) and (sum(y_pred[i]) == 0):\n",
    "            continue\n",
    "        temp+= (2*sum(np.logical_and(y_true[i], y_pred[i])))/ (sum(y_true[i])+sum(y_pred[i]))\n",
    "    return temp/ y_true.shape[0]\n",
    "    \n",
    "print('F1 Measure:',F1Measure(y_true, y_pred))\n",
    "print('F1 Measure: {0}'.format(sklearn.metrics.f1_score(y_true=y_true, y_pred=y_pred, average='samples')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./files/multi-label-metrics/macro_f1_02.png\">\n",
    "<img src=\"./files/multi-label-metrics/macro_f1.png\">\n",
    "<img src=\"./files/multi-label-metrics/macro_f1_03.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro_F1: 0.30952380952380953\n",
      "Macro_F1: 0.30952380952380953\n"
     ]
    }
   ],
   "source": [
    "def Macro_F1(y_true,y_pred):\n",
    "    temp =0\n",
    "    n = y_true.shape[0] # nmber of samples \n",
    "    q = y_pred.shape[1] # number of labels \n",
    "    for j in range(q):\n",
    "        temp = temp + 2*(sum(y_true[:,j] * y_pred[:,j])/(sum(y_true[:,j])+sum(y_pred[:,j])))\n",
    "    return (1/q) * temp \n",
    "    \n",
    "print('Macro_F1:' ,Macro_F1(y_true,y_pred))\n",
    "print('Macro_F1: {0}'.format(sklearn.metrics.f1_score(y_true=y_true, y_pred=y_pred, average='macro')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./files/multi-label-metrics/micro_f1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro_F1: 0.42105263157894735\n",
      "Micro_F1: 0.4210526315789474\n"
     ]
    }
   ],
   "source": [
    "def Micro_F1(y_true,y_pred):\n",
    "    temp1 =0\n",
    "    temp2 =0\n",
    "    temp3 =0\n",
    "    n = y_true.shape[0] # nmber of samples \n",
    "    for i in range(n):\n",
    "        temp1 = temp1 + sum( np.logical_and(y_true[i,:] , y_pred[i,:]))\n",
    "        temp2 = temp2 + sum(y_true[i,:])\n",
    "        temp3 = temp3 + sum(y_pred[i,:])\n",
    "        \n",
    "    return 2 * temp1 / (temp2 + temp3)\n",
    "    \n",
    "print('Micro_F1:' ,Micro_F1(y_true,y_pred))\n",
    "print('Micro_F1: {0}'.format(sklearn.metrics.f1_score(y_true=y_true, y_pred=y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Micro_F1(y_true,y_pred):\n",
    "#     temp =0\n",
    "#     n = y_true.shape[0] # nmber of samples \n",
    "#     for i in range(n):\n",
    "#         temp = temp + ( 2 * ( sum( np.logical_and(y_true[i,:] , y_pred[i,:]) )) / (sum(y_true[i,:])+sum(y_pred[i,:])))\n",
    "#     return temp /n\n",
    "    \n",
    "# print('Micro_F1:' ,Micro_F1(y_true,y_pred))\n",
    "# print('Micro_F1: {0}'.format(sklearn.metrics.f1_score(y_true=y_true, y_pred=y_pred, average='samples')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "691e1011f8712cbf5cc6ef55bb65c5f80c8f52cc35f6e4ef7a0d9cc672d9c22c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
