# Unsupervised learning

- The computer is trained with unlabeled data.

- Here there’s no teacher at all, actually the computer might be able to teach you new things after it learns patterns in data, these algorithms a particularly useful in cases where the human expert doesn’t know what to look for in the data.

- are the family of machine learning algorithms which are mainly used in pattern detection and descriptive modeling. However, there are no output categories or labels here based on which the algorithm can try to model relationships. These algorithms try to use techniques on the input data to mine for rules, detect patterns, and summarize and group the data points which help in deriving meaningful insights and describe the data better to the users.

## List of Common Task and Algorithms:
- ## Clustering Algorithms 
    (the task of grouping similar instances into clusters) 
    
    - K-Means
    - Hierarchical Clustering and Spectral Clustering
    - DBSCAN and OPTICS
    - Density Peak Clustering
    - Affinity Propagation
    - Mean Shift and BIRCH
    - Gaussian Mixture Models

- ## Association Rules
    
    Association Rule Learning (also called Association Rules or simply Association) is another unsupervised learning task. It is most often used in business analysis to maximize profits.

    It aims to detect unobvious relationships between variables in a dataset, so also can be considered as a data analysis tool. There are many complex algorithms to solve it, but the most popular are:

    - Apriori — based on breadth-first search
    - Eclat (Equivalence Class Transformation) — based on depth-first search
    - FP-Growth— designed to detect frequently occurring patterns in the data
    
    A common example of such a task is product placement. For example, knowing that people often buy onions together with potatoes in supermarkets, it makes sense to place them side by side to increase sales. Therefore, associative rules are used in promotional pricing, marketing, continuous production, etc.

- ## Density Estimation

    Density Estimation is the task of estimating the density of the distribution of data points. More formally, it estimates the probability density function (PDF) of the random process that is generated by the given dataset. This task historically came from statistics, when it was necessary to estimate the PDF of some random variable and can be solved using statistical approaches.

    In the modern era, it is used mostly for data analysis and as an auxiliary tool for anomaly detection — data points located in regions of low density are more likely to be anomalies or outliers. Now it is usually solved with density-based clustering algorithms such as DBSCAN or Mean Shift, and using Expectation-Maximization algorithm into Gaussian Mixture Models.

- ## Dimensionality Reduction 
    (the task of reducing the number of input features in a dataset)
  
    - Principal Component Analysis
    - Manifold Learning — LLE, Isomap, t-SNE
    - Autoencoders


- ## Anomaly Detection 
    (the task of detecting instances or patterns that are very different from the norm)
    
    - Isolation Forest
    - Local Outlier Factor
    - Minimum Covariance Determinant and other algorithms from dimensionality reduction or supervised learning